{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: up, Next State: 100, Reward: 100, Done: False Current Health: 100\n",
      "Action: left, Next State: 94, Reward: 94, Done: False Current Health: 94\n",
      "Action: right, Next State: 66, Reward: 66, Done: False Current Health: 66\n",
      "Action: left, Next State: 48, Reward: 48, Done: False Current Health: 48\n",
      "Action: left, Next State: 27, Reward: 27, Done: False Current Health: 27\n",
      "Action: right, Next State: 0, Reward: 0, Done: True Current Health: 0\n",
      "Total Reward: 335\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class HealthBarEnvironment:\n",
    "    def __init__(self, initial_health=100, max_health=100, min_health=0):\n",
    "        self.initial_health = initial_health\n",
    "        self.max_health = max_health\n",
    "        self.min_health = min_health\n",
    "        self.current_health = initial_health\n",
    "        self.action_space = ['up', 'down', 'left', 'right']\n",
    "        self.state_space = 1  # We'll keep it simple with a 1-dimensional state space for health\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_health = self.initial_health\n",
    "        return self.current_health\n",
    "\n",
    "    def step(self, action):\n",
    "        assert action in self.action_space, \"Invalid action\"\n",
    "\n",
    "        # Define the effects of each action on health\n",
    "        if action == 'up':\n",
    "            # Randomly increase health\n",
    "            self.current_health += np.random.randint(1, 10)\n",
    "        elif action == 'down':\n",
    "            # Randomly decrease health\n",
    "            self.current_health -= np.random.randint(1, 10) * 2\n",
    "        elif action == 'left':\n",
    "            self.current_health -= np.random.randint(1, 10) * 3\n",
    "            pass\n",
    "        elif action == 'right':\n",
    "            self.current_health -= np.random.randint(1, 10) * 4\n",
    "            pass\n",
    "\n",
    "        # Clip the health value to stay within the min and max health bounds\n",
    "        self.current_health = np.clip(\n",
    "            self.current_health, self.min_health, self.max_health)\n",
    "\n",
    "        # Calculate the reward\n",
    "        reward = self.calculate_reward()\n",
    "\n",
    "        # Check if the episode is done (health reaches minimum)\n",
    "        done = self.current_health <= self.min_health\n",
    "\n",
    "        # Additional info can be returned if needed\n",
    "        info = {}\n",
    "\n",
    "        return self.current_health, reward, done, info\n",
    "\n",
    "    def calculate_reward(self):\n",
    "        # In this simple example, the reward is simply the current health value\n",
    "        return self.current_health\n",
    "\n",
    "\n",
    "# Let's test our environment\n",
    "env = HealthBarEnvironment()\n",
    "state = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "while not done:\n",
    "    action = np.random.choice(env.action_space)\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "    print(\n",
    "        f\"Action: {action}, Next State: {next_state}, Reward: {reward}, Done: {done} Current Health: {env.current_health}\")\n",
    "\n",
    "print(f\"Total Reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = []\n",
    "        self.gamma = 0.95  # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam())\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state,verbose=0)\n",
    "        return np.argmax(act_values[0])  # returns action\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = self.model.predict(state, verbose=0)\n",
    "            if done:\n",
    "                target[0][action] = reward\n",
    "            else:\n",
    "                Q_future = max(self.model.predict(next_state,verbose=0)[0])\n",
    "                target[0][action] = reward + Q_future * self.gamma\n",
    "            self.model.fit(state, target, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0/50, score: 154, e: 1.0\n",
      "episode: 1/50, score: 528, e: 1.0\n",
      "episode: 2/50, score: 380, e: 1.0\n",
      "episode: 3/50, score: 411, e: 0.98\n",
      "episode: 4/50, score: 754, e: 0.92\n",
      "episode: 5/50, score: 1071, e: 0.84\n",
      "episode: 6/50, score: 344, e: 0.81\n",
      "episode: 7/50, score: 794, e: 0.73\n",
      "episode: 8/50, score: 639, e: 0.68\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Initialize the DQN agent\n",
    "state_size = env.state_space\n",
    "action_size = len(env.action_space)\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "# Number of episodes for agent to play through\n",
    "episodes = 50\n",
    "\n",
    "# Batch size for agent to learn from\n",
    "batch_size = 32\n",
    "\n",
    "for e in range(episodes):\n",
    "    # Reset state at the start of each new episode of the game\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        # Agent takes action\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(env.action_space[action])\n",
    "        reward = reward if not done else -10\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        # print('action:', action, 'reward:', reward, 'next_state:', next_state, 'done:', done)\n",
    "        # Remember the previous state, action, reward, and done\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "\n",
    "        # make next_state the new current state for the next frame.\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        # done becomes True when the game ends\n",
    "        if done:\n",
    "            print(\n",
    "                f\"episode: {e}/{episodes}, score: {total_reward}, e: {agent.epsilon:.2}\")\n",
    "\n",
    "        # train the agent with the experience of the episode\n",
    "        if len(agent.memory) > batch_size:\n",
    "            agent.replay(batch_size)\n",
    "\n",
    "print(f\"Total Reward: {total_reward}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
